import stopwords from '../data/stopwords.json' assert { type: 'json' };

export function tokenizeInput(text) {
  // Step 1: Capture the raw input string from the UI surface.
  const raw = text ?? '';
  // Step 2: Normalize whitespace and collapse repeated spaces.
  const squashed = raw.replace(/\s+/g, ' ').trim();
  // Step 3: Lowercase the content for case-insensitive matching.
  const lower = squashed.toLowerCase();
  // Step 4: Strip punctuation that is not meaningful for concept tokens.
  const stripped = lower.replace(/[^\p{L}\p{N}\s]/gu, '');
  // Step 5: Split the cleaned string into token-sized fragments.
  const parts = stripped.split(' ');
  // Step 6: Remove empty fragments generated by consecutive spaces.
  const filtered = parts.filter(Boolean);
  // Step 7: Exclude stopwords to focus on semantic carriers.
  const coreTokens = filtered.filter((token) => !stopwords.includes(token));
  // Step 8: Count token frequency for later weighting.
  const frequency = new Map();
  for (const token of coreTokens) {
    frequency.set(token, (frequency.get(token) || 0) + 1);
  }
  // Step 9: Sort tokens deterministically to maintain stable layout IDs.
  const sortedTokens = [...frequency.entries()].sort((a, b) => b[1] - a[1] || a[0].localeCompare(b[0]));
  // Step 10: Produce normalized token objects with base scores.
  return sortedTokens.map(([text, count], index) => ({
    id: `tok-${index}`,
    text,
    count,
    weight: 1 + Math.log2(1 + count)
  }));
}
